<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Face UI</title>
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0-rc.2/css/materialize.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0-rc.2/js/materialize.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/vue@2.5.17/dist/vue.js"></script>
    <script src="https://use.fontawesome.com/releases/v5.3.1/js/all.js" data-auto-replace-svg="nest"></script>
    <script src="/js/face-api.js"></script>
    <style>
        .camera {
            width: 600px; height: 400px; margin-top: 2vh; padding: 0 !important;
            overflow: hidden; margin-right: auto; margin-left: auto; position: relative;
        }

        .brand-logo svg { margin-top: auto; margin-bottom: auto;  display: block; margin-top: 0.5vh; }

        video, canvas { width: 600px; height: 400px; }

        canvas { position: absolute; top: 0; left: 0; }

        .model-progress, .app { margin-top: 3vh; }
        .modal { width: 420px; }

        @media only screen and (max-width: 768px) {
            /* For mobile phones: */
            video, canvas { width: 100%; height: unset; }
            .camera { width: unset; height: unset; }
            .modal { width: 80%; }
        }

    </style>
</head>

<body>
    <!--Page header-->
    <nav>
        <div class="nav-wrapper blue">
            <a href="#" class="brand-logo center pulse"><i class="far fa-grin-tongue-wink fa-2x"></i>Face Recognition</a>
        </div>
    </nav>

    <div class="container">
        <div class="row">
            <div class="col s12 m8 offset-m2 center">
                <!-- Video display -->
                <div class="camera">
                    <video id="video" width="400" height="300" preload="auto" loop playsinline autoplay></video>
                    <canvas id="canvas" width="400" height="300"></canvas>
                </div>
            </div>
            <!--Loading model progress bar-->
            <div class="col s12 m6 offset-m3 center model-progress">
                Loading Models
                <div class="progress">
                    <div class="indeterminate"></div>
                </div>
            </div>

            <div class="col s12 m6 offset-m3  app">
                <!-- Floating add image button -->
                <div class="fixed-action-btn">
                    <a id="menu" class="waves-effect waves-light btn-large btn btn-floating blue" v-on:click="addImage" ><i class="material-icons">camera</i></a>
                </div>
                <!-- Display trained faces -->
                <div class="col s12">
                    <div class="chips" v-if="images.length">
                        <div class="chip" v-for="image in images">
                                <img :src="image.url" :alt="image.name"> {{image.name}}
                            <i class="close material-icons" @click="remove(image)">close</i>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <script>
        // Face detection config
        let scoreThreshold = 0.5
        let sizeType = '320'

        // Querying all required elements from the DOM
        const videoElement = document.querySelector('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        
        // Variable to store detection boxes array
        let faceCoords = null;
        
        // Starting detection on play
        videoElement.onplay = () => { 
            videoElement.play();
            detectFaces();
        };
     
        // Simple Vue app to databind trained faces
        const app = new Vue({
            el: '.app',
            data: {
                images: [],
                trackFace: null
            },
            methods: {
                addImage: async function() {
                    const image = {};
                    if (faceCoords && faceCoords.length) {
                        const detect = faceCoords[0];
                        const cvs = getCanvas(detect);
                        const url = cvs.toDataURL('image/jpeg', 0.5);
                        const detector = await faceapi.computeFaceDescriptor(cvs)
                        image.url = url;
                        image.detector = detector;
                        image.name = prompt('face name') || 'Unknown';
                        this.images.push(image);
                    }
                },
                remove: function(image) {
                    this.trackFace = !!this.images.length;
                    this.images.splice(this.images.indexOf(image), 1)
                }
            }
        });

        function getCanvas(detect) {
            const cvs = document.createElement('canvas');
            const cts = cvs.getContext('2d');
            cvs.width = detect.box.width;
            cvs.height = detect.box.height;
            cts.drawImage(videoElement, detect.box.x, detect.box.y, detect.box.width, detect.box.height, 0, 0, detect.box.width, detect.box.height);
            return cvs;
        }

        function startVideo() {
            navigator.mediaDevices.getUserMedia({video : true})
            .then(stream => videoElement.srcObject = stream,
                err => console.log('ERROR: ', err));
        }

        async function detectFaces() {
            /*const { width, height } = faceapi.getMediaDimensions(videoElement);
            canvas.width = width;
            canvas.height = height;
            
            const forwardParams = {
                inputSize: 'sm',
                scoreThreshold
            };

            const result = await faceapi.tinyYolov2(videoElement, forwardParams);
            const detectionsForSize = result.map(det => det.forSize(width, height))
            faceapi.drawDetection(canvas, detectionsForSize, { withScore: false });
            
            // Check if there is some training faces available
            if (app.images.length){
                
            }

            faceCoords = detectionsForSize;
            requestAnimationFrame(detectFaces);*/
        }

        async function loadModels() {
            await faceapi.loadTinyYolov2Model('/models');
            await faceapi.loadFaceRecognitionModel('/models');
            document.querySelector('.model-progress').classList.add('hide');
            startVideo();
        }

        loadModels();
        
    </script>
</body>
</html>